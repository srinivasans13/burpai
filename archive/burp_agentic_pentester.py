"""
Agentic Burp Suite Pentesting AI Agent
This agent autonomously performs penetration testing with full request/response logging
Supports both Anthropic (Claude) and Ollama LLM providers
"""

import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union
import requests
from requests.structures import CaseInsensitiveDict


class OllamaClient:
    """
    Ollama API client for local LLM inference.
    Supports function calling with compatible models like llama3.2, mistral, etc.
    """
    
    def __init__(self, base_url: str = "http://localhost:11434", model: str = "minimax-m2.5:cloud"):
        self.base_url = base_url.rstrip('/')
        self.model = model
        self.session = requests.Session()
    
    def chat(self, messages: List[Dict], tools: Optional[List[Dict]] = None, 
             temperature: float = 0.7, max_tokens: int = 4096) -> Dict:
        """
        Send a chat request to Ollama with optional tools (function calling)
        """
        url = f"{self.base_url}/api/chat"
        
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "stream": False
        }
        
        if tools:
            payload["tools"] = tools
        
        try:
            response = self.session.post(url, json=payload, timeout=120)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.ConnectionError:
            raise ConnectionError(f"Could not connect to Ollama at {self.base_url}. Is Ollama running?")
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Ollama API error: {str(e)}")


class RequestLogger:
    """Handles all request/response logging with detailed formatting"""
    
    def __init__(self, log_file: str = "pentest_requests.log"):
        self.log_file = log_file
        self.request_counter = 0
        
    def log_request(self, 
                   method: str, 
                   url: str, 
                   headers: Dict, 
                   body: Optional[str],
                   purpose: str,
                   expected: str,
                   response_status: int,
                   response_headers: Dict,
                   response_body: str,
                   analysis: str,
                   next_step: str) -> str:
        """Log a complete request-response cycle"""
        
        self.request_counter += 1
        timestamp = datetime.now().strftime("%Y-%m-%d-%H%M%S")
        request_id = f"{timestamp}-{self.request_counter:03d}"
        
        log_entry = f"""
{'='*80}
[REQUEST ID: {request_id}]
TIMESTAMP: {datetime.now().isoformat()}
METHOD: {method}
URL: {url}

HEADERS:
{self._format_headers(headers)}

BODY:
{body if body else '(empty)'}

PURPOSE: {purpose}
EXPECTED: {expected}

{'---RESPONSE---':-^80}
STATUS: {response_status}

RESPONSE HEADERS:
{self._format_headers(response_headers)}

RESPONSE BODY (First 1000 chars):
{response_body[:1000]}
{f'... ({len(response_body)} total chars)' if len(response_body) > 1000 else ''}

ANALYSIS: {analysis}
NEXT_STEP: {next_step}
{'='*80}

"""
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry)
        
        return request_id
    
    def _format_headers(self, headers: Dict) -> str:
        """Format headers for readable logging"""
        if not headers:
            return "(no headers)"
        return "\n".join([f"  {k}: {v}" for k, v in headers.items()])


class AgenticPentestAgent:
    """
    Autonomous pentesting agent that uses LLM (Ollama or Claude) to make intelligent
    testing decisions with full transparency
    """
    
    def __init__(self, 
                 target_base_url: str, 
                 proxy: Optional[str] = None,
                 provider: str = "ollama",
                 api_key: Optional[str] = None,
                 ollama_base_url: str = "http://localhost:11434",
                 ollama_model: str = "minimax-m2.5:cloud"):
        """
        Initialize the agent with the specified LLM provider.
        
        Args:
            target_base_url: Base URL of the target application
            proxy: Optional proxy URL (e.g., for Burp Suite)
            provider: LLM provider - "ollama" or "anthropic"
            api_key: API key for Anthropic (if using anthropic provider)
            ollama_base_url: Base URL for Ollama server
            ollama_model: Model name to use with Ollama
        """
        self.target_base_url = target_base_url
        self.provider = provider
        self.logger = RequestLogger()
        self.conversation_history = []
        self.vulnerabilities_found = []
        
        # Configure proxy for Burp Suite integration
        self.proxies = {"http": proxy, "https": proxy} if proxy else None
        
        # System prompt for the AI agent
        self.system_prompt = self._load_system_prompt()
        
        # Initialize the appropriate LLM client
        if provider == "ollama":
            self.llm = OllamaClient(base_url=ollama_base_url, model=ollama_model)
            print(f"ü§ñ Using Ollama provider: {ollama_model} at {ollama_base_url}")
        elif provider == "anthropic":
            import anthropic
            if not api_key:
                raise ValueError("api_key is required for anthropic provider")
            self.llm = anthropic.Anthropic(api_key=api_key)
            print(f"ü§ñ Using Anthropic provider with Claude")
        else:
            raise ValueError(f"Unknown provider: {provider}. Use 'ollama' or 'anthropic'")
    
    def _load_system_prompt(self) -> str:
        """Load the agentic pentesting system prompt"""
        try:
            with open('burp-ai-agent-prompt.md', 'r') as f:
                return f.read()
        except FileNotFoundError:
            # Fallback embedded prompt
            return """You are an expert autonomous penetration testing AI agent.
            
Your mission: Systematically discover and exploit web vulnerabilities with full transparency.

For EVERY request you make, you must:
1. Explain WHY you're making it
2. State what you EXPECT to learn
3. Make the actual HTTP request
4. Analyze the response
5. Decide your NEXT action based on findings

Test for: SQLi, XSS, IDOR, Auth bypasses, Business logic flaws, API vulnerabilities.

Be intelligent, adaptive, and thorough. Log everything.

IMPORTANT: When you need to make HTTP requests or report vulnerabilities, use the available tools.
Format your responses clearly and be very specific about what you're testing."""
    
    def _convert_history_to_provider_format(self) -> List[Dict]:
        """
        Convert conversation history to the format expected by the current provider
        """
        converted = []
        for msg in self.conversation_history:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            # Handle different content types
            if isinstance(content, list):
                # Handle tool results - convert to text
                tool_text = ""
                for item in content:
                    if item.get("type") == "tool_result":
                        tool_text += f"\n[Tool Result]:\n{item.get('content', '')}\n"
                content = tool_text
            elif hasattr(content, 'text'):
                # Handle Anthropic content blocks
                content = content.text
            
            converted.append({"role": role, "content": str(content)})
        
        return converted
    
    def _get_tools_definition(self) -> List[Dict]:
        """Define tools for the agent to use"""
        return [
            {
                "name": "execute_http_request",
                "description": "Execute an HTTP request against the target application. Use this to test for vulnerabilities, gather information, or exploit findings. You MUST explain the purpose and expected outcome for each request.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "method": {
                            "type": "string",
                            "enum": ["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS", "HEAD"],
                            "description": "HTTP method"
                        },
                        "endpoint": {
                            "type": "string",
                            "description": "API endpoint or path (e.g., /api/login, /users/1)"
                        },
                        "headers": {
                            "type": "object",
                            "description": "HTTP headers as key-value pairs"
                        },
                        "body": {
                            "type": "string",
                            "description": "Request body (JSON, form data, etc.)"
                        },
                        "purpose": {
                            "type": "string",
                            "description": "WHY you're making this request - what are you testing?"
                        },
                        "expected": {
                            "type": "string",
                            "description": "What you EXPECT to discover or learn from this request"
                        }
                    },
                    "required": ["method", "endpoint", "purpose", "expected"]
                }
            },
            {
                "name": "report_vulnerability",
                "description": "Report a confirmed vulnerability with details, impact, and remediation",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string",
                            "description": "Vulnerability name (e.g., SQL Injection in login endpoint)"
                        },
                        "severity": {
                            "type": "string",
                            "enum": ["Critical", "High", "Medium", "Low", "Info"],
                            "description": "Severity level"
                        },
                        "cvss_score": {
                            "type": "number",
                            "description": "CVSS score (0-10)"
                        },
                        "location": {
                            "type": "string",
                            "description": "Where the vulnerability exists"
                        },
                        "description": {
                            "type": "string",
                            "description": "Detailed description of the vulnerability"
                        },
                        "impact": {
                            "type": "string",
                            "description": "What an attacker could achieve"
                        },
                        "poc": {
                            "type": "string",
                            "description": "Proof of concept - the exact request that demonstrates it"
                        },
                        "remediation": {
                            "type": "string",
                            "description": "How to fix the vulnerability"
                        },
                        "request_id": {
                            "type": "string",
                            "description": "Reference to the logged request that discovered this"
                        }
                    },
                    "required": ["name", "severity", "location", "description", "impact", "poc", "remediation"]
                }
            }
        ]
    
    def execute_http_request(self, 
                           method: str, 
                           endpoint: str, 
                           headers: Optional[Dict] = None,
                           body: Optional[str] = None,
                           purpose: str = "",
                           expected: str = "") -> Dict:
        """
        Execute an HTTP request and log everything
        Returns the response details for the AI to analyze
        """
        
        url = f"{self.target_base_url}{endpoint}"
        headers = headers or {}
        
        try:
            # Make the actual request
            response = requests.request(
                method=method,
                url=url,
                headers=headers,
                data=body,
                proxies=self.proxies,
                verify=False,  # For testing environments
                timeout=30
            )
            
            # Prepare response data
            response_data = {
                "status_code": response.status_code,
                "headers": dict(response.headers),
                "body": response.text[:5000],  # Limit response size
                "elapsed_ms": response.elapsed.total_seconds() * 1000
            }
            
            # Log to file (analysis and next_step will be added by AI)
            request_id = self.logger.log_request(
                method=method,
                url=url,
                headers=headers,
                body=body,
                purpose=purpose,
                expected=expected,
                response_status=response.status_code,
                response_headers=dict(response.headers),
                response_body=response.text,
                analysis="(pending AI analysis)",
                next_step="(pending AI decision)"
            )
            
            response_data["request_id"] = request_id
            return response_data
            
        except Exception as e:
            error_response = {
                "status_code": 0,
                "error": str(e),
                "headers": {},
                "body": f"Request failed: {str(e)}"
            }
            return error_response
    
    def _process_ollama_response(self, response: Dict) -> Tuple[str, Optional[Dict]]:
        """
        Process Ollama response and handle tool calls
        Returns (response_text, tool_call or None)
        """
        message = response.get("message", {})
        content = message.get("content", "")
        
        # Check for tool calls in Ollama format
        tool_calls = message.get("tool_calls", [])
        
        if tool_calls:
            tool_call = tool_calls[0]  # Handle first tool call
            return content, {
                "name": tool_call.get("function", {}).get("name"),
                "arguments": tool_call.get("function", {}).get("arguments", {}),
                "id": tool_call.get("id", "ollama_tool")
            }
        
        return content, None
    
    def chat_with_agent(self, user_message: str) -> str:
        """
        Send a message to the AI agent and get its response
        The agent will autonomously decide what requests to make
        """
        
        # Add user message to history
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })
        
        # Get tools definition
        tools = self._get_tools_definition()
        
        # Convert history to provider format
        messages = self._convert_history_to_provider_format()
        
        # Add system prompt as first message if using Ollama
        if self.provider == "ollama":
            messages.insert(0, {"role": "system", "content": self.system_prompt})
        
        response_text = ""
        
        # Main loop for handling tool calls
        max_iterations = 20  # Prevent infinite loops
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            
            if self.provider == "ollama":
                try:
                    response = self.llm.chat(
                        messages=messages,
                        tools=tools,
                        max_tokens=4096
                    )
                except ConnectionError as e:
                    return f"Error: {str(e)}\n\nMake sure Ollama is running on your system."
                
                content, tool_call = self._process_ollama_response(response)
                response_text += content + "\n\n"
                
                if not tool_call:
                    break  # No more tool calls, we're done
                    
            elif self.provider == "anthropic":
                import anthropic
                response = self.llm.messages.create(
                    model="claude-sonnet-4-20250514",
                    max_tokens=4096,
                    system=self.system_prompt,
                    messages=self.conversation_history,
                    tools=tools
                )
                
                # Extract text content
                for block in response.content:
                    if block.type == "text":
                        response_text += block.text + "\n\n"
                
                # Check for tool use
                tool_blocks = [block for block in response.content if block.type == "tool_use"]
                if not tool_blocks:
                    break  # No more tool calls
                
                tool_call = {
                    "name": tool_blocks[0].name,
                    "arguments": tool_blocks[0].input,
                    "id": tool_blocks[0].id
                }
            
            # Process the tool call
            print(f"\nüîß Agent is executing tool: {tool_call['name']}")
            
            if tool_call["name"] == "execute_http_request":
                args = tool_call["arguments"]
                print(f"   {args.get('method')} {args.get('endpoint')}")
                print(f"   Purpose: {args.get('purpose')}")
                
                result = self.execute_http_request(
                    method=args.get("method"),
                    endpoint=args.get("endpoint"),
                    headers=args.get("headers"),
                    body=args.get("body"),
                    purpose=args.get("purpose"),
                    expected=args.get("expected")
                )
                
                # Add assistant's tool call to history
                self.conversation_history.append({
                    "role": "assistant",
                    "content": f"[Calling tool: {tool_call['name']} with arguments: {json.dumps(args)}]"
                })
                
                # Add tool result to history
                self.conversation_history.append({
                    "role": "user",
                    "content": f"[Tool Result]:\n{json.dumps(result, indent=2)}"
                })
                
                # Add to messages for next iteration
                messages = self._convert_history_to_provider_format()
                if self.provider == "ollama":
                    messages.insert(0, {"role": "system", "content": self.system_prompt})
                    
            elif tool_call["name"] == "report_vulnerability":
                args = tool_call["arguments"]
                self.vulnerabilities_found.append(args)
                print(f"\nüéØ VULNERABILITY REPORTED: {args.get('name')} [{args.get('severity')}]")
                
                # Add to history
                self.conversation_history.append({
                    "role": "assistant",
                    "content": f"[Calling tool: {tool_call['name']} with arguments: {json.dumps(args)}]"
                })
                
                self.conversation_history.append({
                    "role": "user",
                    "content": "Vulnerability logged successfully"
                })
                
                messages = self._convert_history_to_provider_format()
                if self.provider == "ollama":
                    messages.insert(0, {"role": "system", "content": self.system_prompt})
            else:
                # Unknown tool, add error and continue
                self.conversation_history.append({
                    "role": "user",
                    "content": f"Error: Unknown tool '{tool_call['name']}'. Please use available tools."
                })
                messages = self._convert_history_to_provider_format()
                if self.provider == "ollama":
                    messages.insert(0, {"role": "system", "content": self.system_prompt})
        
        if iteration >= max_iterations:
            response_text += "\n\n[Warning: Maximum iterations reached. Stopping to prevent infinite loop.]"
        
        # Add final response to history
        self.conversation_history.append({
            "role": "assistant",
            "content": response_text
        })
        
        return response_text
    
    def generate_report(self) -> str:
        """Generate a final penetration testing report"""
        
        report = f"""
{'='*80}
PENETRATION TESTING REPORT
{'='*80}

Target: {self.target_base_url}
Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
Total Requests Made: {self.logger.request_counter}
Vulnerabilities Found: {len(self.vulnerabilities_found)}
LLM Provider: {self.provider.upper()}

{'='*80}
EXECUTIVE SUMMARY
{'='*80}

"""
        
        severity_counts = {
            "Critical": 0,
            "High": 0,
            "Medium": 0,
            "Low": 0,
            "Info": 0
        }
        
        for vuln in self.vulnerabilities_found:
            severity_counts[vuln.get("severity", "Info")] += 1
        
        report += f"""
Critical: {severity_counts['Critical']}
High: {severity_counts['High']}
Medium: {severity_counts['Medium']}
Low: {severity_counts['Low']}
Informational: {severity_counts['Info']}

{'='*80}
DETAILED FINDINGS
{'='*80}

"""
        
        for i, vuln in enumerate(self.vulnerabilities_found, 1):
            report += f"""
{'‚îÄ'*80}
{i}. {vuln.get('name')}
{'‚îÄ'*80}
Severity: {vuln.get('severity')}
CVSS Score: {vuln.get('cvss_score', 'N/A')}
Location: {vuln.get('location')}

DESCRIPTION:
{vuln.get('description')}

IMPACT:
{vuln.get('impact')}

PROOF OF CONCEPT:
{vuln.get('poc')}

REMEDIATION:
{vuln.get('remediation')}

Request Log Reference: {vuln.get('request_id', 'N/A')}

"""
        
        return report


# Example usage
if __name__ == "__main__":
    # Configuration - Using Ollama
    TARGET_URL = "https://demo.testfire.net"
    BURP_PROXY = "http://127.0.0.1:8080"  # Optional: route through Burp
    
    # Ollama configuration
    OLLAMA_BASE_URL = "http://localhost:11434"
    OLLAMA_MODEL = "minimax-m2.5:cloud"  # Change to your model
    
    # Initialize the agent with Ollama
    agent = AgenticPentestAgent(
        target_base_url=TARGET_URL,
        proxy=BURP_PROXY,
        provider="ollama",
        ollama_base_url=OLLAMA_BASE_URL,
        ollama_model=OLLAMA_MODEL
    )
    
    print("ü§ñ Agentic Pentesting AI Agent Initialized")
    print(f"üéØ Target: {TARGET_URL}")
    print(f"üìù Logging to: pentest_requests.log")
    print("="*80)
    
    # Start autonomous pentesting
    response = agent.chat_with_agent(
        "Perform a comprehensive penetration test on this application. "
        "Start with reconnaissance, then systematically test for OWASP Top 10 vulnerabilities. "
        "Be thorough, intelligent, and document everything."
    )
    
    print("\n" + response)
    
    # Interactive mode
    print("\n\nüí¨ Entering interactive mode. Type 'report' to generate final report, 'exit' to quit.\n")
    
    while True:
        user_input = input("\nYou: ").strip()
        
        if user_input.lower() == 'exit':
            break
        elif user_input.lower() == 'report':
            report = agent.generate_report()
            print(report)
            with open('pentest_report.txt', 'w') as f:
                f.write(report)
            print("\n‚úÖ Report saved to pentest_report.txt")
        else:
            response = agent.chat_with_agent(user_input)
            print(f"\nü§ñ Agent: {response}")
    
    # Final report
    print("\n\nüìä Generating final report...")
    final_report = agent.generate_report()
    with open('pentest_report.txt', 'w') as f:
        f.write(final_report)
    
    print("‚úÖ Penetration test complete!")
    print(f"üìÑ Final report: pentest_report.txt")
    print(f"üìù Request logs: pentest_requests.log")
