package com.burpai.aipentester.clients;

import burp.api.montoya.logging.Logging;
import com.fasterxml.jackson.databind.ObjectMapper;

import java.io.IOException;
import java.net.Proxy;
import java.net.ProxySelector;
import java.net.SocketAddress;
import java.net.URI;
import java.net.http.*;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;
import com.burpai.aipentester.llm.LlmClient;

/**
 * LLM client targeting the OpenAI REST API directly.
 *
 * <p>Default model: {@code gpt-4o}
 *
 * <p>API reference: <a href="https://platform.openai.com/docs/api-reference/chat">
 * https://platform.openai.com/docs/api-reference/chat</a>
 *
 * <p>Uses the standard OpenAI chat completions endpoint with native function/tool calling.
 * For reasoning models (o1, o3) that have restricted tool call support, a text-mode fallback
 * is applied automatically — the client permanently switches to inline JSON tool-call parsing
 * if the API returns a 400 or 404 indicating the model does not support native tool use.
 */
public class OpenAIClient implements LlmClient {

  private static final String CHAT_URL   = "https://api.openai.com/v1/chat/completions";
  private static final String MODELS_URL = "https://api.openai.com/v1/models";

  private final String model;
  private final String apiKey;
  private final Logging logging;
  private final HttpClient http;
  private final ObjectMapper json = new ObjectMapper();

  /** Flipped permanently when the model doesn't support native function calling. */
  private volatile boolean toolsStripped = false;

  /**
   * @param model  OpenAI model ID, e.g. {@code "gpt-4o"}, {@code "gpt-4o-mini"}, {@code "o3"}
   * @param apiKey OpenAI API key (sk-…)
   */
  public OpenAIClient(String model, String apiKey, Logging logging) {
    this.model   = model.trim();
    this.apiKey  = apiKey.trim();
    this.logging = logging;
    this.http = HttpClient.newBuilder()
        .followRedirects(HttpClient.Redirect.NORMAL)
        .proxy(new DirectProxySelector())   // bypass Burp proxy for external API calls
        .connectTimeout(Duration.ofSeconds(20))
        .build();
  }

  // Force direct connections — avoids Burp intercepting calls to api.openai.com.
  private static final class DirectProxySelector extends ProxySelector {
    @Override public List<Proxy> select(URI uri) { return List.of(Proxy.NO_PROXY); }
    @Override public void connectFailed(URI uri, SocketAddress sa, IOException ioe) {}
  }

  // ── LlmClient ─────────────────────────────────────────────────────────────

  @Override
  public ConnResult testConnection() {
    try {
      HttpRequest req = HttpRequest.newBuilder()
          .uri(URI.create(MODELS_URL))
          .timeout(Duration.ofSeconds(15))
          .header("Authorization", "Bearer " + apiKey)
          .GET()
          .build();

      HttpResponse<String> resp = sendWithTimeout(req, Duration.ofSeconds(20));

      if (resp.statusCode() == 401) {
        return new ConnResult(false,
            "OpenAI API key is invalid (HTTP 401). Check your key at https://platform.openai.com/api-keys");
      }
      if (resp.statusCode() == 403) {
        return new ConnResult(false,
            "OpenAI API key lacks permission (HTTP 403).");
      }
      if (resp.statusCode() != 200) {
        return new ConnResult(false,
            "OpenAI /models returned HTTP " + resp.statusCode()
            + ": " + extractError(resp.body()));
      }

      // Verify the requested model ID appears in the catalogue.
      boolean found = resp.body().contains("\"" + model + "\"");
      if (!found) {
        return new ConnResult(false,
            "API key is valid but model '" + model + "' was not found. "
            + "Check available models at https://platform.openai.com/docs/models");
      }
      return new ConnResult(true,
          "Connected to OpenAI API. Key valid. Model '" + model + "' is available.");

    } catch (Exception e) {
      return new ConnResult(false,
          "Connection error: " + e.getClass().getSimpleName() + ": " + e.getMessage());
    }
  }

  @Override
  public ChatResult chat(List<Map<String, Object>> messages, List<Map<String, Object>> tools) {
    int attempts = 0;
    while (true) {
      attempts++;
      try {
        List<Map<String, Object>> sendMessages = toolsStripped ? stripToolMessages(messages) : messages;
        List<Map<String, Object>> sendTools    = toolsStripped ? List.of() : tools;

        Map<String, Object> payload = new LinkedHashMap<>();
        payload.put("model", model);
        payload.put("messages", sendMessages);
        if (!sendTools.isEmpty()) {
          payload.put("tools", sendTools);
          payload.put("tool_choice", "auto");
        }

        String body = json.writeValueAsString(payload);
        logging.logToOutput("[OpenAIClient] POST " + CHAT_URL
            + " (msgs=" + sendMessages.size() + ", body=" + body.length()
            + " bytes, toolsStripped=" + toolsStripped + ")");

        HttpRequest req = buildRequest(body, Duration.ofSeconds(180));
        HttpResponse<String> resp = sendWithTimeout(req, Duration.ofSeconds(185));
        logging.logToOutput("[OpenAIClient] HTTP " + resp.statusCode());

        // ── Model doesn't support tool calling → switch to text mode ─────
        int sc = resp.statusCode();
        if ((sc == 400 || sc == 404) && !toolsStripped
            && resp.body() != null
            && (resp.body().contains("tool") || resp.body().contains("function"))) {
          logging.logToError("[OpenAIClient] Model does not support native tool calling — "
              + "switching permanently to text-mode tool calling.");
          toolsStripped = true;
          attempts--; // don't count against retry limit
          continue;
        }

        // ── 401 / 403: bad key ────────────────────────────────────────────
        if (sc == 401 || sc == 403) {
          return ChatResult.err("OpenAI: Invalid or expired API key (HTTP " + sc + "). "
              + "Check https://platform.openai.com/api-keys");
        }

        // ── Retryable errors ──────────────────────────────────────────────
        // 429 Too Many Requests / rate limit, 500 Internal, 503 Unavailable
        if (sc == 429 || sc == 500 || sc == 503) {
          if (attempts >= 4) {
            return ChatResult.err("OpenAI HTTP " + sc + " after " + attempts
                + " attempts: " + extractError(resp.body()));
          }
          long backoffMs = attempts * 3000L;
          logging.logToError("[OpenAIClient] HTTP " + sc
              + " (retryable); retrying in " + backoffMs + "ms (attempt " + attempts + ")");
          Thread.sleep(backoffMs);
          continue;
        }

        if (sc != 200) {
          return ChatResult.err("OpenAI HTTP " + sc + ": " + extractError(resp.body()));
        }

        // ── Parse response ────────────────────────────────────────────────
        @SuppressWarnings("unchecked")
        Map<String, Object> data = json.readValue(resp.body(), Map.class);

        @SuppressWarnings("unchecked")
        List<Object> choices = (List<Object>) data.get("choices");
        if (choices == null || choices.isEmpty()) {
          return ChatResult.err("OpenAI response missing 'choices': " + truncate(resp.body(), 300));
        }

        @SuppressWarnings("unchecked")
        Map<String, Object> choice = (Map<String, Object>) choices.get(0);
        @SuppressWarnings("unchecked")
        Map<String, Object> msg = (Map<String, Object>) choice.get("message");
        if (msg == null) {
          return ChatResult.err("OpenAI 'message' missing in choice");
        }

        Object rawContent = msg.get("content");
        String content = rawContent != null ? String.valueOf(rawContent) : "";

        // ── Tool calls ────────────────────────────────────────────────────
        List<LlmClient.ToolCall> toolCalls;
        if (toolsStripped) {
          toolCalls = parseToolCallsFromText(content);
        } else {
          toolCalls = parseToolCalls(msg);
        }

        return ChatResult.ok(content, toolCalls);

      } catch (InterruptedException ie) {
        Thread.currentThread().interrupt();
        return ChatResult.err("Interrupted during OpenAI call");

      } catch (java.net.ConnectException | java.net.SocketTimeoutException | HttpTimeoutException e) {
        if (attempts >= 3) {
          return ChatResult.err("OpenAI connect/timeout after " + attempts
              + " attempts: " + e.getMessage());
        }
        long backoffMs = attempts * 1500L;
        logging.logToError("[OpenAIClient] Transient failure (attempt " + attempts
            + "): " + e + "; retrying in " + backoffMs + "ms");
        try {
          Thread.sleep(backoffMs);
        } catch (InterruptedException ie2) {
          Thread.currentThread().interrupt();
          return ChatResult.err("Interrupted while retrying OpenAI call");
        }

      } catch (Throwable t) {
        logging.logToError("[OpenAIClient] chat failed: " + t);
        String msg2 = t.getMessage();
        if (msg2 == null || msg2.isBlank()) msg2 = String.valueOf(t);
        return ChatResult.err(t.getClass().getName() + ": " + msg2);
      }
    }
  }

  // ── Text-mode helpers (used when model doesn't support native tool calls) ──

  /**
   * Converts the message history to a form safe for models without native tool support.
   * Re-wraps {@code role:tool} messages as {@code role:user} messages and injects a
   * system instruction for inline JSON tool-call format.
   */
  @SuppressWarnings("unchecked")
  private static List<Map<String, Object>> stripToolMessages(List<Map<String, Object>> messages) {
    List<Map<String, Object>> out = new ArrayList<>();

    Map<String, Object> instruction = new HashMap<>();
    instruction.put("role", "system");
    instruction.put("content",
        "TOOL CALLING MODE: This model does not support native function calling. "
        + "Whenever you want to call a tool, emit a JSON object in your response text with exactly this shape:\n"
        + "{\"name\":\"<tool_name>\",\"arguments\":{<args>}}\n"
        + "Available tools: execute_http_request, report_vulnerability, extract_from_response, "
        + "get_variable, set_variable, fuzz_parameter, decode_encode, search_in_response, "
        + "spider_links, get_sitemap, finish_run\n"
        + "Do NOT use markdown fences. Place the JSON on its own line.");
    out.add(instruction);

    for (Map<String, Object> m : messages) {
      String role = String.valueOf(m.getOrDefault("role", ""));
      if ("tool".equals(role)) {
        String name    = String.valueOf(m.getOrDefault("name", "tool"));
        String content = String.valueOf(m.getOrDefault("content", ""));
        Map<String, Object> wrapped = new HashMap<>();
        wrapped.put("role", "user");
        wrapped.put("content", "[Tool result for " + name + "]\n" + content);
        out.add(wrapped);
      } else {
        if ("system".equals(role)) {
          String c = String.valueOf(m.getOrDefault("content", ""));
          if (c.startsWith("TOOL CALLING MODE:")) continue; // deduplicate
        }
        out.add(m);
      }
    }
    return out;
  }

  /**
   * Scans {@code content} for bare JSON objects that look like tool calls
   * ({@code "name"} and {@code "arguments"} keys) and returns them.
   */
  @SuppressWarnings("unchecked")
  private List<LlmClient.ToolCall> parseToolCallsFromText(String content) {
    List<LlmClient.ToolCall> result = new ArrayList<>();
    if (content == null || content.isBlank()) return result;

    int depth = 0;
    int start = -1;
    for (int i = 0; i < content.length(); i++) {
      char c = content.charAt(i);
      if (c == '{') {
        if (depth == 0) start = i;
        depth++;
      } else if (c == '}') {
        depth--;
        if (depth == 0 && start >= 0) {
          String candidate = content.substring(start, i + 1);
          try {
            Object parsed = json.readValue(candidate, Object.class);
            if (parsed instanceof Map<?, ?> m) {
              Map<String, Object> map = (Map<String, Object>) m;
              String name = null;
              Object argsObj = null;

              if (map.containsKey("name") && map.containsKey("arguments")) {
                name    = String.valueOf(map.get("name"));
                argsObj = map.get("arguments");
              }

              if (name != null && isKnownTool(name)) {
                Map<String, Object> args = new HashMap<>();
                if (argsObj instanceof Map<?, ?> am) {
                  args = (Map<String, Object>) am;
                } else if (argsObj instanceof String s) {
                  try {
                    Object p = json.readValue(s, Object.class);
                    if (p instanceof Map<?, ?> pm) args = (Map<String, Object>) pm;
                  } catch (Exception ignored) {}
                }
                result.add(new LlmClient.ToolCall(name, args));
              }
            }
          } catch (Exception ignored) {}
          start = -1;
        }
      }
    }
    return result;
  }

  private static boolean isKnownTool(String name) {
    return switch (name) {
      case "execute_http_request", "report_vulnerability", "extract_from_response",
           "get_variable", "set_variable", "fuzz_parameter", "decode_encode",
           "search_in_response", "spider_links", "get_sitemap", "finish_run" -> true;
      default -> false;
    };
  }

  // ── Native tool-call parsing ───────────────────────────────────────────────

  @SuppressWarnings("unchecked")
  private List<LlmClient.ToolCall> parseToolCalls(Map<String, Object> msg) {
    List<LlmClient.ToolCall> result = new ArrayList<>();
    Object tcObj = msg.get("tool_calls");
    if (!(tcObj instanceof List<?> list)) return result;

    for (Object o : list) {
      if (!(o instanceof Map<?, ?> tc)) continue;
      Object fnObj = tc.get("function");
      if (!(fnObj instanceof Map<?, ?> fn)) continue;

      String id   = tc.get("id") != null ? String.valueOf(tc.get("id")) : null;
      String name = fn.get("name") != null ? String.valueOf(fn.get("name")) : null;
      if (name == null || name.isBlank()) continue;

      Object argsObj = fn.get("arguments");
      Map<String, Object> args = new HashMap<>();
      if (argsObj instanceof String s) {
        try {
          Object parsed = json.readValue(s, Object.class);
          if (parsed instanceof Map<?, ?> pm) args = (Map<String, Object>) pm;
        } catch (Exception ignored) {}
      } else if (argsObj instanceof Map<?, ?> am) {
        args = (Map<String, Object>) am;
      }
      result.add(new LlmClient.ToolCall(id, name, args));
    }
    return result;
  }

  // ── Helpers ────────────────────────────────────────────────────────────────

  private HttpRequest buildRequest(String body, Duration timeout) {
    return HttpRequest.newBuilder()
        .uri(URI.create(CHAT_URL))
        .timeout(timeout)
        .header("Content-Type", "application/json")
        .header("Authorization", "Bearer " + apiKey)
        .POST(HttpRequest.BodyPublishers.ofString(body))
        .build();
  }

  private String extractError(String body) {
    if (body == null || body.isBlank()) return "(empty body)";
    try {
      @SuppressWarnings("unchecked")
      Map<String, Object> map = json.readValue(body, Map.class);
      Object err = map.get("error");
      if (err instanceof Map<?, ?> em) {
        Object msg = em.get("message");
        if (msg != null) return String.valueOf(msg);
      }
      if (err != null) return String.valueOf(err);
    } catch (Exception ignored) {}
    return truncate(body, 300);
  }

  private static String truncate(String s, int max) {
    if (s == null) return "";
    return s.length() <= max ? s : s.substring(0, max) + "...[truncated]";
  }

  private HttpResponse<String> sendWithTimeout(HttpRequest req, Duration timeout) throws Exception {
    var fut = http.sendAsync(req, HttpResponse.BodyHandlers.ofString());
    try {
      return fut.get(timeout.toMillis(), TimeUnit.MILLISECONDS);
    } catch (TimeoutException te) {
      fut.cancel(true);
      throw new HttpTimeoutException("OpenAI request timed out after " + timeout.toSeconds() + "s");
    } catch (ExecutionException ee) {
      Throwable cause = ee.getCause();
      if (cause instanceof Exception e) throw e;
      throw new RuntimeException(cause);
    } catch (InterruptedException ie) {
      Thread.currentThread().interrupt();
      throw ie;
    }
  }
}
